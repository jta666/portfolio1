{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YqUj3GDh_Vj",
        "outputId": "dcb5e56a-e1ac-4684-ee3c-fd0f7374a3e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Firstly we'll mount the drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Then import some stuff.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "8KA2ueQyiNFf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a global size constant.\n",
        "SIZE = 64"
      ],
      "metadata": {
        "id": "rmgSd3uBiYEv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And read the data.\n",
        "data = pd.read_csv('/content/drive/My Drive/SUS/bigtask2/data/train/train.csv')"
      ],
      "metadata": {
        "id": "nGzaoCKQ83YC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then process it a little.\n",
        "def process_path(path):\n",
        "    base_path = \"/content/drive/My Drive/SUS/bigtask2/data\"\n",
        "    new_path = path.removeprefix(\"BigDataCup2022/S1\")\n",
        "    return base_path + new_path\n",
        "\n",
        "full_image_paths = [process_path(path) for path in data[\"input_path\"]]\n",
        "encoded_image_paths = [process_path(path) for path in data[\"encoded_path\"]]"
      ],
      "metadata": {
        "id": "KtFuVBZQ81Wk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "import concurrent.futures\n",
        "\n",
        "def process_image(path):\n",
        "    with Image.open(path) as image:\n",
        "        resized_image = resize_image(image)\n",
        "        return np.array(resized_image)\n",
        "\n",
        "# To make the task more managable, we'll squash the images a little.\n",
        "# Luckily there exists an amazing resizing filter called LANCZOS, which\n",
        "# takes care of keeping everything that is important.\n",
        "def resize_image(image):\n",
        "    prev_width = image.size[0]\n",
        "    prev_height = image.size[1]\n",
        "    resampling_coefficient = (SIZE / float(prev_width))\n",
        "    new_height = int((float(prev_height) * resampling_coefficient))\n",
        "    return image.resize((SIZE, new_height), Image.Resampling.LANCZOS)\n",
        "\n",
        "# As reading from files is the performance barrier, we'll use a lot of threads.\n",
        "def get_processed_images(paths):\n",
        "    results = [None] * len(paths)\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:\n",
        "        future_to_index = {executor.submit(process_image, path): index for index, path in enumerate(paths)}\n",
        "        for future in concurrent.futures.as_completed(future_to_index):\n",
        "            result = future.result()\n",
        "            index = future_to_index[future]\n",
        "            results[index] = result\n",
        "            # print(index)\n",
        "    return results\n",
        "\n",
        "# full_images = np.array(get_processed_images(full_image_paths))\n",
        "# encoded_images = np.array(get_processed_images(encoded_image_paths))\n",
        "\n",
        "# # full_images = np.save('/content/drive/My Drive/SUS/bigtask2/train_full_images_64.npy')\n",
        "# # encoded_images = np.save('/content/drive/My Drive/SUS/bigtask2/train_encoded_images_64.npy')\n",
        "\n",
        "full_images = np.load('/content/drive/My Drive/SUS/bigtask2/train_full_images_64.npy')\n",
        "encoded_images = np.load('/content/drive/My Drive/SUS/bigtask2/train_encoded_images_64.npy')\n",
        "\n",
        "# Once the images are processed, we'll create our dataset.\n",
        "\n",
        "data_length = len(full_images)\n",
        "\n",
        "# We'll create positive pairs.\n",
        "pairs_with_markers = [(full_images[i], encoded_images[i], 1) for i in range(data_length)]\n",
        "\n",
        "# And negative pairs.\n",
        "for i in range(data_length):\n",
        "    j = np.random.randint(data_length)\n",
        "    # We need to ensure that the images here are actually different.\n",
        "    if j == i:\n",
        "        j = (j + 55) % data_length\n",
        "    pairs_with_markers.append((full_images[i], encoded_images[i], 0))\n",
        "\n",
        "# We'll mix the examples.\n",
        "np.random.shuffle(pairs_with_markers)\n",
        "\n",
        "# And create actual training data.\n",
        "training_pairs = np.array([np.concatenate((x, y)) for x, y, _ in pairs_with_markers])\n",
        "training_markers = np.array([z for _, _, z in pairs_with_markers])\n",
        "\n",
        "np.save('/content/drive/My Drive/SUS/bigtask2/training_pairs.npy', training_pairs)\n",
        "np.save('/content/drive/My Drive/SUS/bigtask2/training_markers.npy', training_markers)"
      ],
      "metadata": {
        "id": "9llME-b0_CX2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we'll create the model. Thanks to the experience I gained when writing my\n",
        "# bachelor thesis and through some hit or misses, I came up with the following model.\n",
        "# It was a CNN model from the start, but its exact shape was created thorugh trial and error.\n",
        "# For example batch normalization turned out to be much more important than\n",
        "# I've ever thought before!\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, BatchNormalization\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(16, (3, 3), input_shape=(SIZE * 2, SIZE, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(16, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "CVJUZhoeitJ-"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can fit the model.\n",
        "training_pairs = np.load('/content/drive/My Drive/SUS/bigtask2/training_pairs.npy')\n",
        "training_markers = np.load('/content/drive/My Drive/SUS/bigtask2/training_markers.npy')\n",
        "\n",
        "# We'll train it for 8 epochs, which seems to get it to the verge of overfitting.\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(training_pairs, training_markers, batch_size=32, epochs=8, verbose=1)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pFpFa-N-i0Pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, let's process the competition data.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "competition_data = pd.read_csv('/content/drive/My Drive/SUS/bigtask2/data/test/test.csv')\n",
        "\n",
        "competition_full_image_paths = [process_path(path) for path in competition_data[\"input_path\"]]\n",
        "competition_encoded_image_paths = [process_path(path) for path in competition_data[\"encoded_path\"]]\n",
        "\n",
        "input_images = np.array(get_processed_images(competition_full_image_paths))\n",
        "encoded_images = np.array(get_processed_images(competition_encoded_image_paths))\n",
        "\n",
        "competition_pairs = np.hstack((input_images, encoded_images))\n",
        "\n",
        "np.save('/content/drive/My Drive/SUS/bigtask2/competition_pairs.npy', competition_pairs)"
      ],
      "metadata": {
        "id": "EuP6hnFHAXUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And generate the predicitons.\n",
        "competition_pairs = np.load('/content/drive/My Drive/SUS/bigtask2/competition_pairs.npy')\n",
        "\n",
        "predictions_raw = model.predict(competition_pairs)\n",
        "print(predictions_raw[:100])\n",
        "\n",
        "predictions = [0 if x <= 0.5 else 1 for x in predictions_raw]\n",
        "print(predictions[:100])\n",
        "\n",
        "with open('/content/drive/My Drive/SUS/bigtask2/final_predictions.txt', 'w') as file:\n",
        "    for prediction in predictions:\n",
        "        file.write(f\"{prediction}\\n\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pgeb5v5BMMGD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}